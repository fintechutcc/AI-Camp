{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fintechutcc/AI-Camp/blob/main/Basic_TensorFlow_with_Text_Lab_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C3ItRAa8--G"
      },
      "source": [
        "# Basic Tensor Flow with Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-2Hkj6u86iH"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c345bKIZ676c"
      },
      "source": [
        "ตัวอย่างประโยคอยู่ในลิสต์ sentences ได้แก่ 'I love my dog', 'I love my cat', 'You love my dog!', 'Do you think my dog is amazing?'\n",
        "\n",
        "เมื่อใช้ Tokenizer ของ TensorFlow สามารถกำหนดจำนวนศัพท์ในคลังศัพท์ (Corpus) ให้เป็นจำนวนใดๆ ในกรณีนี้กำหนดให้มีคลังศัพท์เพียง 100 คำ\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "```\n",
        "\n",
        "ฟังก์ชัน fit_on_texts เป็นการนำศัพท์ที่อยู่ประโยคตัวอย่าง มาจัดดัชนี (Index) ดังผลลัพธ์ด้านล่างนี้\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mts_Rm5A9H9Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3af25771-e4a6-472b-f7a5-793f00e0d6b7"
      },
      "source": [
        "sentences = ['I love my dog', 'I love my cat', 'You love my dog!', 'Do you think my dog is amazing?']\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzDEVXG48R0-"
      },
      "source": [
        "ผลลัพธ์ข้างบนนี้เป็นรายการคำศัพท์ในคลังศัพท์พร้อมกับดัชนี ในกรณีนี้มีเพียง 10 คำ ดังนั้นหากต้องการให้คลังศัพท์มีคำศัพท์จำนวนมาก จำเป็นต้องใช้ประโยคตัวอย่างจำนวนมากนั่นเอง\n",
        "\n",
        "ในกรณีกลับกัน หากนำประโยคตัวอย่างมาแสดงในรูปของลำดับดัชนีคำศัพท์ (Sequence) จะได้ผลลัพธ์ด้านล่างนี้ เช่น I love my dog จะได้ผลลัพธ์ของ Sequence เป็น [4, 2, 1, 3] เป็นต้น"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V654H8pS_a_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b273cdc-6521-4359-e2c9-e490b75ccbf5"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(word_index)\n",
        "print(sequences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
            "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DSXCJY1Afy8"
      },
      "source": [
        "## Missing words\n",
        "\n",
        "ผลลัพธ์ของ Sequence ข้างต้นจะมีปัญหา หากคลังศัพท์ไม่มีคำศัพท์ที่ต้องการ เช่น ในกรณีตัวอย่างด้านล่างนี้ 'I really love my dog' ได้ผลลัพธ์ของการทำ Sequence เหมือนกับ 'I love my dog' คือ [4, 2, 1, 3] เนื่องจากคำ 'really' ไม่ได้อยู่ในคลังศัพท์"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thwh7LfFAjw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7273674-d30c-4dc0-9855-f1f1c36bcf20"
      },
      "source": [
        "test_data = ['I really love my dog', 'My dog loves my shoes']\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(test_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4, 2, 1, 3], [1, 3, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2UG31g-A-R-"
      },
      "source": [
        "นี่เป็นเหตุผลหนึ่ง ที่เราต้องการจำนวนคำศัพท์ในคลังศัพท์จำนวนมาก แล้วหากกรณีนี้จะแก้ปัญหาการไม่มีคำศัพท์ในคลังได้อย่างไร?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUeLsFsYBJJi"
      },
      "source": [
        "## OOV - Out of Vocabulary\n",
        "ลองย้อนกลับไปที่โค้ดนี้\n",
        "\n",
        "`tokenizer = Tokenizer(num_words = 100)`\n",
        "\n",
        "แล้วเปลี่ยนเป็น\n",
        "\n",
        "`tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")`\n",
        "\n",
        "แล้วรันใหม่ทั้งหมดทุกเซลล์ สังเกตผลลัพธ์ที่ได้"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuddxIGJRh_2"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n",
        "\n",
        "test_data = ['I really love my dog', 'My dog loves my shoes']\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(test_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLfSH-08vBMS",
        "outputId": "a7b6df5a-61cf-4e59-c237-51ce63b4addf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyQjzPfgC9Ws"
      },
      "source": [
        "## Padding\n",
        "Padding หรือ การเติมเต็ม ใช้เพื่อให้ผลลัพธ์ของ Sequence ของแต่ละประโยคมีขนาดเท่ากัน โดยการเติม 0 ลงไป อาจเติมด้านท้าย หรือเติมด้านหน้าก็ได้ ในกรณีตัวอย่างนี้เติม 0 ด้านท้าย"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzn-15VpC_j9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97989b58-8f3b-4c63-8a48-8cf69ab1716c"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padded = pad_sequences(sequences, padding='post')\n",
        "print(sequences)\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n",
            "[[ 4  2  1  3  0  0  0]\n",
            " [ 4  2  1  6  0  0  0]\n",
            " [ 5  2  1  3  0  0  0]\n",
            " [ 7  5  8  1  3  9 10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxEphAq__NiK"
      },
      "source": [
        "## Summary\n",
        "\n",
        "สิ่งที่ทำมาทั้งหมดคือ เทคนิคในการแปลงประโยคที่มีตัวอักษรให้เป็นตัวเลข เพื่อให้สามารถนำมาใส่ในสมการคณิตศาสตร์ได้นั่น"
      ]
    }
  ]
}